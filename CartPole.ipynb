{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c252c056-0b0e-48d3-95bc-67a4edf51eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcc72324-a955-47d1-acd4-195bdcf8a808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable_baselines3[extra] in c:\\programdata\\anaconda3\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: gymnasium<1.3.0,>=0.29.1 in c:\\users\\sudha\\appdata\\roaming\\python\\python313\\site-packages (from stable_baselines3[extra]) (1.2.1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.20 in c:\\programdata\\anaconda3\\lib\\site-packages (from stable_baselines3[extra]) (2.1.3)\n",
      "Requirement already satisfied: torch<3.0,>=2.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from stable_baselines3[extra]) (2.9.0)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\sudha\\appdata\\roaming\\python\\python313\\site-packages (from stable_baselines3[extra]) (3.1.1)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (from stable_baselines3[extra]) (2.2.3)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (from stable_baselines3[extra]) (3.10.0)\n",
      "Requirement already satisfied: opencv-python in c:\\programdata\\anaconda3\\lib\\site-packages (from stable_baselines3[extra]) (4.12.0.88)\n",
      "Requirement already satisfied: pygame in c:\\programdata\\anaconda3\\lib\\site-packages (from stable_baselines3[extra]) (2.6.1)\n",
      "Requirement already satisfied: tensorboard>=2.9.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from stable_baselines3[extra]) (2.20.0)\n",
      "Requirement already satisfied: psutil in c:\\programdata\\anaconda3\\lib\\site-packages (from stable_baselines3[extra]) (5.9.0)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from stable_baselines3[extra]) (4.67.1)\n",
      "Requirement already satisfied: rich in c:\\programdata\\anaconda3\\lib\\site-packages (from stable_baselines3[extra]) (13.9.4)\n",
      "Requirement already satisfied: ale-py>=0.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from stable_baselines3[extra]) (0.11.2)\n",
      "Requirement already satisfied: pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from stable_baselines3[extra]) (11.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gymnasium<1.3.0,>=0.29.1->stable_baselines3[extra]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\sudha\\appdata\\roaming\\python\\python313\\site-packages (from gymnasium<1.3.0,>=0.29.1->stable_baselines3[extra]) (0.0.4)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (3.17.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch<3.0,>=2.3->stable_baselines3[extra]) (72.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch<3.0,>=2.3->stable_baselines3[extra]) (1.3.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (2.3.1)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (1.76.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (3.8)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (5.29.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable_baselines3[extra]) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->stable_baselines3[extra]) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->stable_baselines3[extra]) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->stable_baselines3[extra]) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->stable_baselines3[extra]) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->stable_baselines3[extra]) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->stable_baselines3[extra]) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3[extra]) (1.17.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->stable_baselines3[extra]) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->stable_baselines3[extra]) (2025.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->stable_baselines3[extra]) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->stable_baselines3[extra]) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->stable_baselines3[extra]) (0.1.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->stable_baselines3[extra]) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install stable_baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8ffdae6-8b1f-4558-98b7-7cf62bfb06cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea64547a-05fe-4a0a-987e-53cfd5e3f4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make('CartPole-v1', render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0d84a1f-b3d1-4920-b438-930e0c70f3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Score: 17.0\n",
      "Episode: 2, Score: 31.0\n",
      "Episode: 3, Score: 14.0\n",
      "Episode: 4, Score: 9.0\n",
      "Episode: 5, Score: 17.0\n"
     ]
    }
   ],
   "source": [
    "episodes=5\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    state=env.reset() # We get the initial set of the environment\n",
    "    done=False\n",
    "    score=0\n",
    "\n",
    "    while not done:\n",
    "        env.render() # View the graphical visualisation of the environment \n",
    "        action=env.action_space.sample()\n",
    "        n_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done=terminated or truncated\n",
    "        score+=reward\n",
    "        \n",
    "    print(f\"Episode: {episode}, Score: {score}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19118cef-7938-4ad9-b25f-e84c2f856366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Two actions are there:\n",
    "0. Push the cart to the left\n",
    "1. Push the cart to the right\n",
    "'''\n",
    "\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71201818-c3eb-461d-94b5-91f58929ecd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample() # Action space example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf135f93-1b0a-4c7e-85a5-4c147c2b5450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "There are 4 observations given along with their min and max value\n",
    "NUM.      Observation                 MIN                     MAX\n",
    "0.        Cart Position               -4.8                    4.8\n",
    "1.        Cart Velocity               -INF                    INF\n",
    "2.        Pole Angle                  -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
    "3.        Pole Angular Velocity       -INF                    INF\n",
    "'''\n",
    "\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f45fba6-c468-4f8d-b4d1-29c7f954eebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.49791777,  0.01082975,  0.05925753,  0.49490732], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample() # Observation space example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6788a6f7-e392-43d3-bfb0-f18ed9980593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu126\n",
      "Requirement already satisfied: torch in c:\\programdata\\anaconda3\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: torchvision in c:\\programdata\\anaconda3\\lib\\site-packages (0.24.0+cu126)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (2.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77a8f11c-0952-432e-9ba9-1b4d862f1957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make your Directories\n",
    "log_path = os.path.join('Training', 'Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ab146cd-f60e-48fb-ab86-2d5407c0e66e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\Logs'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0cb1cab8-fedc-47f2-8f24-d92c7c8212db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "env=gym.make('CartPole-v1', render_mode=\"human\")\n",
    "env=DummyVecEnv([lambda: env])\n",
    "# Multi layer Perceptron policy i.e, using deep NN policy\n",
    "model=PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path) # We can use tensor-board to visualise metrics to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d68d1a82-3f45-42ec-9ba5-03c64512e1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_9\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 46   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 43   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 89          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009255767 |\n",
      "|    clip_fraction        | 0.0998      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | -0.00112    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.41        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0155     |\n",
      "|    value_loss           | 57.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 45           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 134          |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0081305625 |\n",
      "|    clip_fraction        | 0.0542       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.669       |\n",
      "|    explained_variance   | 0.0807       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.9         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.0165      |\n",
      "|    value_loss           | 44.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 179         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009114071 |\n",
      "|    clip_fraction        | 0.0813      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.634      |\n",
      "|    explained_variance   | 0.235       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.6        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.019      |\n",
      "|    value_loss           | 55.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 224         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007948578 |\n",
      "|    clip_fraction        | 0.0618      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.611      |\n",
      "|    explained_variance   | 0.339       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 36.7        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0151     |\n",
      "|    value_loss           | 67.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 45           |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 269          |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055272244 |\n",
      "|    clip_fraction        | 0.0396       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.591       |\n",
      "|    explained_variance   | 0.429        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 24.5         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0077      |\n",
      "|    value_loss           | 63           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 45           |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 314          |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041272733 |\n",
      "|    clip_fraction        | 0.016        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.583       |\n",
      "|    explained_variance   | 0.632        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.57         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.0045      |\n",
      "|    value_loss           | 49.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 45           |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 358          |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044066403 |\n",
      "|    clip_fraction        | 0.0483       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.575       |\n",
      "|    explained_variance   | 0.708        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.15         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00684     |\n",
      "|    value_loss           | 45.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 45           |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 403          |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014353498 |\n",
      "|    clip_fraction        | 0.0168       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.586       |\n",
      "|    explained_variance   | 0.835        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.75         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00178     |\n",
      "|    value_loss           | 23.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 448         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008817503 |\n",
      "|    clip_fraction        | 0.0834      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.572      |\n",
      "|    explained_variance   | 0.869       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.8        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00808    |\n",
      "|    value_loss           | 28.7        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x2a2fb62a350>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000)\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "916cd757-b228-4dbe-82d7-df09652506b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_Path=os.path.join('Training', 'Saved Models', 'PPO_Model_CartPole')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2811d7e1-7a7f-4b2b-a7e6-62e00bd92c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(PPO_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd4a5297-1468-4846-867a-6b757cb69b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e28e682-ad2e-46e9-9833-fd61c5708463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\Saved Models\\\\PPO_Model_CartPole'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PPO_Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2103285d-fb2a-4bd0-a3f1-fcef8643d1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=PPO.load(PPO_Path, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d810e30-9037-460a-b31a-01681a0ea3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\miniconda3\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:70: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(500.0), np.float64(0.0))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54070076-c04e-4f0d-9235-48bde13f8d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cab32c43-7c14-404c-877b-50599ba93a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "obs, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "155bb0f7-9206-4ecf-8080-70ed03dbcbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "action, _=model.predict(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "42448b04-e8a6-412f-b485-c2ea85118ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Score: 476.0\n",
      "Episode: 2, Score: 500.0\n",
      "Episode: 3, Score: 324.0\n",
      "Episode: 4, Score: 500.0\n",
      "Episode: 5, Score: 291.0\n"
     ]
    }
   ],
   "source": [
    "episodes=5\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    obs, info=env.reset()\n",
    "    done=False\n",
    "    score=0\n",
    "\n",
    "    while not done:\n",
    "        env.render() # View the graphical visualisation of the environment \n",
    "        action, _=model.predict(obs) # Here we are using model now\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done=terminated or truncated\n",
    "        score+=reward\n",
    "        \n",
    "    print(f\"Episode: {episode}, Score: {score}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8dba9ccf-6a9e-4010-9ab4-ee0dd32bd6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0d5b1d8e-af8c-4f01-a75d-e22ca58df44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path=os.path.join('Training', 'Saved Models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2742369d-24e9-427f-8962-9863452670b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env=gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8aefdb73-e27f-47fb-b18b-65dc67ba4b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_callback=StopTrainingOnRewardThreshold(reward_threshold=500, verbose=1)\n",
    "eval_callback=EvalCallback(train_env, callback_on_new_best=stop_callback, eval_freq=10000, best_model_save_path=save_path, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "512f6713-23dd-4d33-9920-f189bb8358d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model=PPO('MlpPolicy', train_env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "550ab4c4-9903-4895-a027-e91370bf0e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_10\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22       |\n",
      "|    ep_rew_mean     | 22       |\n",
      "| time/              |          |\n",
      "|    fps             | 668      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 26          |\n",
      "|    ep_rew_mean          | 26          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 385         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008928662 |\n",
      "|    clip_fraction        | 0.0978      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | -0.000201   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.61        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    value_loss           | 51.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 33.2        |\n",
      "|    ep_rew_mean          | 33.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 376         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010843126 |\n",
      "|    clip_fraction        | 0.0814      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.67       |\n",
      "|    explained_variance   | 0.101       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.3        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0207     |\n",
      "|    value_loss           | 34.8        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 44.7       |\n",
      "|    ep_rew_mean          | 44.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 378        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 21         |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00904804 |\n",
      "|    clip_fraction        | 0.106      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.633     |\n",
      "|    explained_variance   | 0.264      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 25.4       |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0214    |\n",
      "|    value_loss           | 53.1       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=364.80 +/- 128.36\n",
      "Episode length: 364.80 +/- 128.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 365         |\n",
      "|    mean_reward          | 365         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007413066 |\n",
      "|    clip_fraction        | 0.0573      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.613      |\n",
      "|    explained_variance   | 0.228       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.5        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0149     |\n",
      "|    value_loss           | 65.3        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 56.5     |\n",
      "|    ep_rew_mean     | 56.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 350      |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 29       |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 75.8        |\n",
      "|    ep_rew_mean          | 75.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 354         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 34          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009339271 |\n",
      "|    clip_fraction        | 0.0729      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.585      |\n",
      "|    explained_variance   | 0.436       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.6        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0149     |\n",
      "|    value_loss           | 59.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 91.5        |\n",
      "|    ep_rew_mean          | 91.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 355         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 40          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005569148 |\n",
      "|    clip_fraction        | 0.038       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.578      |\n",
      "|    explained_variance   | 0.64        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.6        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00805    |\n",
      "|    value_loss           | 46.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 108         |\n",
      "|    ep_rew_mean          | 108         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 357         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 45          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003652398 |\n",
      "|    clip_fraction        | 0.0487      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.582      |\n",
      "|    explained_variance   | 0.816       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.59        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00392    |\n",
      "|    value_loss           | 29.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 126          |\n",
      "|    ep_rew_mean          | 126          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 359          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 51           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025135134 |\n",
      "|    clip_fraction        | 0.0247       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.564       |\n",
      "|    explained_variance   | 0.784        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 35           |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00388     |\n",
      "|    value_loss           | 38.9         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=497.20 +/- 5.60\n",
      "Episode length: 497.20 +/- 5.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 497          |\n",
      "|    mean_reward          | 497          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042130337 |\n",
      "|    clip_fraction        | 0.0472       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.563       |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.77         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00481     |\n",
      "|    value_loss           | 20.4         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 142      |\n",
      "|    ep_rew_mean     | 142      |\n",
      "| time/              |          |\n",
      "|    fps             | 345      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 59       |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 160         |\n",
      "|    ep_rew_mean          | 160         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 350         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 64          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004324014 |\n",
      "|    clip_fraction        | 0.0409      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.556      |\n",
      "|    explained_variance   | 0.815       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.35        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00446    |\n",
      "|    value_loss           | 36.9        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x2a2fc2efc50>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20500, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d9d0c3-25aa-47ca-80de-3197be732241",
   "metadata": {},
   "source": [
    "# Change Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "97ed3abb-7477-4dbc-ae28-d4639bb20d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_arch=[dict(pi=[128,128,128,128], vf=[128,128,128,128])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0e7b3dd2-ed36-46d5-a5c6-512c15e61521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\miniconda3\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model=PPO('MlpPolicy', train_env, verbose=1, tensorboard_log=log_path, policy_kwargs={'net_arch':net_arch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9b3768bc-81e7-4ace-97c0-7bbf50e6fbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_11\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.1     |\n",
      "|    ep_rew_mean     | 21.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 580      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 27.7        |\n",
      "|    ep_rew_mean          | 27.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 371         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014114851 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.681      |\n",
      "|    explained_variance   | -0.0113     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.45        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0232     |\n",
      "|    value_loss           | 19.7        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 42.4       |\n",
      "|    ep_rew_mean          | 42.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 329        |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 18         |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02011179 |\n",
      "|    clip_fraction        | 0.218      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.643     |\n",
      "|    explained_variance   | 0.445      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 11.5       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0357    |\n",
      "|    value_loss           | 26         |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=7472, episode_reward=254.20 +/- 74.82\n",
      "Episode length: 254.20 +/- 74.82\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 254         |\n",
      "|    mean_reward          | 254         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7472        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012814955 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.596      |\n",
      "|    explained_variance   | 0.423       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.6        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0258     |\n",
      "|    value_loss           | 43.3        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 58.8     |\n",
      "|    ep_rew_mean     | 58.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 293      |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 27       |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 76.2       |\n",
      "|    ep_rew_mean          | 76.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 285        |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 35         |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01316385 |\n",
      "|    clip_fraction        | 0.113      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.572     |\n",
      "|    explained_variance   | 0.566      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 22.9       |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.015     |\n",
      "|    value_loss           | 36.2       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 93.2        |\n",
      "|    ep_rew_mean          | 93.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 285         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010867566 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.554      |\n",
      "|    explained_variance   | 0.823       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.63        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0173     |\n",
      "|    value_loss           | 17.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 111         |\n",
      "|    ep_rew_mean          | 111         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 286         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009860774 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.543      |\n",
      "|    explained_variance   | 0.82        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.8         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00726    |\n",
      "|    value_loss           | 19.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 127         |\n",
      "|    ep_rew_mean          | 127         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 286         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 57          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013676551 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.539      |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.871       |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    value_loss           | 9.08        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17472, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17472       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009016909 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.553      |\n",
      "|    explained_variance   | 0.968       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.61        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.007      |\n",
      "|    value_loss           | 7.08        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Stopping training because the mean reward 500.00  is above the threshold 500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x2a2fc2ef9d0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20500, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603aeccf-37f7-40cb-926f-02a78ad23565",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
